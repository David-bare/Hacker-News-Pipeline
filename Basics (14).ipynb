{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker News Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will use the pipeline which I have worked on and apply it to a real world data pipeline project. From a JSON API, I will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for me.\n",
    "\n",
    "The data I will use comes from a [Hacker News](https://news.ycombinator.com/) (HN) API that returns [JSON data](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON) of the top stories in 2014. If you're unfamiliar with Hacker News, it's a link aggregator website that users vote up stories that are interesting to the community. It is similar to [Reddit](https://www.reddit.com/), but the community only revolves around on computer science and entrepreneurship posts.\n",
    "\n",
    "The file hn_stories_2014.json consists of already downloaded list of JSON posts. The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but I will deal only with the following keys:\n",
    "\n",
    "* created_at: A timestamp of the story's creation time.\n",
    "\n",
    "* created_at_i: A unix epoch timestamp.\n",
    "\n",
    "* url: The URL of the story link.\n",
    "\n",
    "* objectID: The ID of the story.\n",
    "\n",
    "* author: The story's author (username on HN).\n",
    "\n",
    "* points: The number of upvotes the story had.\n",
    "\n",
    "* title: The headline of the post.\n",
    "\n",
    "* num_comments: The number of a comments a post has.\n",
    "\n",
    "Using this dataset, I will run a sequence of basic natural language processing tasks using the Pipeline class. \n",
    "\n",
    "The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pipeline import build_csv, Pipeline\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "import json\n",
    "import io\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will load the JSON file data into Python. The goal is to parse the JSON file into a Python dict object. This can be accomplished using the json module.\n",
    "\n",
    "To load in a file, json exposes a method called json.load() which takes in a python object as the first argument. Using this json.load() method, I'll load the hn_stories_2014.json file as a Python dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the stories as a list of dict objects, I can now operate on them. Let us start by filtering the list of stories to get the most popular stories of the year. \n",
    "\n",
    "Like any social link aggregator site, individual users can post whatever content they want. The reason we want the most popular stories is to ensure that we select stories that were the most talked about during the year. We can filter for popular stories by ensuring they are links (not Ask HN posts), have a good number of points, and have some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.task() funtion that depends on the file_to_json()\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "\n",
    "# function that filters stories with more than 50 points, more than \n",
    "# 1 comment and do not begin with Ask HN.\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "    \n",
    "    return (\n",
    "        story for story in stories\n",
    "        if is_popular(story)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a reduced set of stories, it's time to write these dict objects to a CSV file. The purpose of translating the dictionaries to a CSV is to have a consistent data format when running the later summarizations. By keeping consistent data formats, each of the pipeline tasks will be adaptable with future task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.task() function that depends on the filter_stories() function\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "\n",
    "# call function that writes the filtered JSON stories to a CSV file\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'], datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), story['url'], story['points'], story['title'])\n",
    "        )\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Title Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CSV file format I created in the previous task, I can now extract the title column. Once the titles of each popular post have been extracted, I can then run the next word frequency task. To extract the titles, I'll follow the following steps:\n",
    "\n",
    "1. Import csv, and create a csv.reader() object from the file object. \n",
    "\n",
    "2. Find the index of the title in the header. \n",
    "\n",
    "3. Iterate through the reader, and return each item from the reader in the corresponding title index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.task() function that depends on the json_to_csv()\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "\n",
    "# extract_titles() function that returns a generator of every Hacker\n",
    "# News story title.\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    \n",
    "    return (line[idx] for line in reader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the titles, I will mke the titles lower case and remove the punctuation. An easy way to rid a string of punctuation is to check each character, determine if it is a letter or punctuation, and only keep the letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.task() that depends on extract_titles()\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "\n",
    "# clean_title function returns of a generator of cleaned titles \n",
    "def clean_title(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(c for c in title if c not in string.punctuation)\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Word Frequency Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a cleaned title, I can now build the word frequency dictionary. A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text.\n",
    "\n",
    "Furthermore, to find actual keywords, I will enforce the word frequency dictionary to not include stop words. Stop words are words that occur frequently in language like \"the\", \"or\", etc., and are commonly rejected in keyword searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.task() function that depends on the clean_titles() function.\n",
    "@pipeline.task(depends_on=clean_title)\n",
    "\n",
    "# function that returns a dictionary of the word frequency of all\n",
    "# HN titles.\n",
    "def build_keyword_dictionary(titles):\n",
    "    word_freq = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words:\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Top Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to output a list of tuples with (word, frequency) as the entries sorted from most used, to least most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('apple', 51), ('use', 51), ('make', 51), ('time', 49), ('yc', 49), ('security', 49), ('nsa', 46), ('github', 46), ('windows', 45), ('world', 42), ('way', 42), ('like', 42), ('1', 41), ('project', 41), ('computer', 41), ('heartbleed', 41), ('git', 38), ('users', 38), ('dont', 38), ('design', 38), ('ios', 38), ('developer', 37), ('os', 37), ('twitter', 37), ('ceo', 37), ('vs', 37), ('life', 37), ('big', 36), ('day', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('says', 33), ('apps', 33), ('browser', 33), ('server', 32), ('firefox', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('mozilla', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('amazon', 31), ('year', 31), ('support', 30), ('stop', 30), ('built', 30), ('better', 30), ('million', 30), ('people', 30), ('text', 30), ('3', 29), ('does', 29), ('tech', 29), ('development', 29), ('billion', 28), ('developers', 28), ('just', 28), ('library', 28), ('did', 28), ('website', 28), ('money', 28), ('inside', 28)]\n"
     ]
    }
   ],
   "source": [
    "# pipeline.task() function that depends on the build_keyword_dictionary() function\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "\n",
    "# function that returns a list of the top 100 tuples\n",
    "def top_words(word_freq):\n",
    "    freq_tuple = [\n",
    "        (word, word_freq[word])\n",
    "        for word in sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "    ]\n",
    "    return freq_tuple[:100]\n",
    "\n",
    "# print the output of the new task function\n",
    "ran = pipeline.run()\n",
    "print(ran[top_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returned a list of the top 100 tuples as intended. The output is a list of tuples with (word, frequency) and is sorted from most used to least most used, with the most used word, 'new', occuring 186 times and the least most used word, 'inside', occuring 28 times."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
